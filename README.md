# Gender and Representation Bias in GPT-3 Generated Stories

This README is ordered according to sections in the paper, and each section describes corresponding scripts and materials involved in their production. 

Some TODOs for Lucy by end of May: 

- describe each section below, and what parts of each script correspond to information in the paper 
- upload non-copyrighted intermediate files 

## Abstract
Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3's perceived gender of the character in a prompt, with feminine characters more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.

## Requirements 

See `requirements.txt`. I think I may have switched machines/environments at some point during this project, so let me know if something does not work with these packages. 

## Data

- query\_openai.py: for gathering GPT-3 generated stories
- TODO: a spreadsheet of book titles and other information

We cannot release the original books because they are copyrighted. 

## Text Processing 

- book\_nlp.sh: runs [Book NLP](https://github.com/dbamman/book-nlp) over original books
- check\_book\_bounds.py: just checks that the start and end of books annotated by humans can actually be used to find start/end 
- data\_organize.py: various formatting and sanity checking functions
- dataset\_viz.ipynb: TODO
- get\_characters.py: extracts sentences that mention main characters, to use as prompts 
- get\_entity\_info.py: groups character mentions and finds pronouns associated with them based on coreference chains 
- preprocessing.py: for getting an idea of what books I have
- segment\_original\_books.py: get excerpts from original books with similar length as generated stories 

The coreference model is the same as that used in [Sims et al. 2020](https://github.com/mbwsims/literary-information-propagation/blob/master/run_pipeline.sh).

### Gender 

We do not recommend the use of these methods for inferring the actual gender of real people. 

- gender\_inference.py

TODO: add files that contain characters and pronouns and gender labels 

### Matching

- prompt\_design.py: prompt matching (note: there are some deprecated functions in here, see function comments)

## Topics

- mallet.sh: taken from [this repo](https://github.com/ddemszky/textbook-analysis), runs topic modeling
- get\_topics.py: gets topics for documents/stories, modified from [this repo](https://github.com/ddemszky/textbook-analysis)
- character\_viz.ipynb: TODO

TODO: add topic assignments files 

## Lexicons

- word\_embeddings.py: functions for the word embedding part of this paper 
- prompt\_design.py: getting prompts with specific verbs 
- character\_viz.ipynb: TODO

Ethan Fast's stereotype lexicon is available upon request. Power verbs can be found [here](https://homes.cs.washington.edu/~msap/movie-bias/), Bloom's taxonomy verbs are in the verb folder of this repo, and Empath categories are [here](https://github.com/Ejhfast/empath-client/blob/master/empath/data/categories.tsv). 
